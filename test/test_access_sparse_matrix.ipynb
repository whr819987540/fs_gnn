{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/fs_gnn/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1, 1],\n",
       "                       [2, 0, 2]]),\n",
       "       values=tensor([1., 2., 3.]),\n",
       "       size=(3, 3), nnz=3, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "indices = torch.LongTensor([[0, 1, 1], [2, 0, 2]])\n",
    "values = torch.FloatTensor([1.0, 2.0, 3.0])\n",
    "# 创建稀疏矩阵\n",
    "sparse_matrix = torch.sparse.FloatTensor(indices, values, torch.Size([3, 3]))\n",
    "sparse_matrix = sparse_matrix.coalesce()\n",
    "sparse_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "a = torch.stack((a,b), dim=0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [2., 0., 3.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建稠密矩阵\n",
    "dense_matrix = sparse_matrix.to_dense()\n",
    "dense_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Size((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1, 1],\n",
       "                       [2, 0, 2]]),\n",
       "       values=tensor([2., 2., 3.]),\n",
       "       size=(3, 3), nnz=3, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重复的元素\n",
    "\n",
    "indices = torch.LongTensor([[0, 1, 1, 0], [2, 0, 2, 2]])\n",
    "values = torch.FloatTensor([1.0, 2.0, 3.0, 1.0])\n",
    "# 创建稀疏矩阵\n",
    "sparse_matrix = torch.sparse.FloatTensor(indices, values, torch.Size([3, 3]))\n",
    "sparse_matrix = sparse_matrix.coalesce()\n",
    "sparse_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 2.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [int,:]\n",
    "sparse_matrix[0].to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [:,int]\n",
    "# print(sparse_matrix[:,0].to_dense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [int,int]\n",
    "sparse_matrix[1,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# [tensor,:]\n",
    "print(sparse_matrix.index_select(0, torch.LongTensor([0])).to_dense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [2.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# [:,tensor]\n",
    "print(sparse_matrix.index_select(1, torch.LongTensor([0])).to_dense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# [tensor,tensor]\n",
    "print(sparse_matrix.index_select(0, torch.LongTensor([0,1])).index_select(1, torch.LongTensor([0,1])).to_dense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.sparse_coo\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::index_fill_.int_Tensor' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_fill_.int_Tensor' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/build/aten/src/ATen/RegisterCUDA.cpp:43635 [kernel]\nMeta: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradHIP: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradIPU: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradVE: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sparse_matrix \u001b[39m=\u001b[39m sparse_matrix\u001b[39m.\u001b[39mcoalesce()\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(sparse_matrix\u001b[39m.\u001b[39mlayout)\n\u001b[0;32m----> 3\u001b[0m sparse_matrix\u001b[39m.\u001b[39;49mindex_fill_(\u001b[39m0\u001b[39;49m,torch\u001b[39m.\u001b[39;49mLongTensor([\u001b[39m0\u001b[39;49m]),torch\u001b[39m.\u001b[39;49mLongTensor([\u001b[39m1\u001b[39;49m]))\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::index_fill_.int_Tensor' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_fill_.int_Tensor' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/build/aten/src/ATen/RegisterCUDA.cpp:43635 [kernel]\nMeta: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradHIP: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradIPU: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradVE: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/VariableType_2.cpp:16915 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "sparse_matrix = sparse_matrix.coalesce()\n",
    "print(sparse_matrix.layout)\n",
    "sparse_matrix.index_fill_(0,torch.LongTensor([0]),torch.LongTensor([1]))\n",
    "# sparse_matrix = sparse_matrix.coalesce()\n",
    "# sparse_matrix.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = torch.tensor([[0, 1, 0, 2],[0, 1, 2, 0]])\n",
    "v = torch.tensor([1, 5, 7, 9])\n",
    "\n",
    "sparse_matrix = torch.sparse.IntTensor(i ,v, torch.Size([3, 3]))\n",
    "print(sparse_matrix.to_dense())\n",
    "row_idx = torch.tensor([0, 2])\n",
    "col_idx = torch.tensor([0, 2])\n",
    "\n",
    "selected = sparse_matrix.index_select(0, row_idx).index_select(1, col_idx).coalesce()\n",
    "print(selected)\n",
    "print(selected.to_dense())\n",
    "selected.values().add_(1) # inplace 加1\n",
    "print(selected.to_dense())\n",
    "print(torch.LongTensor([row_idx.tolist(),col_idx.tolist()]))\n",
    "sparse_matrix.index_put_((row_idx,col_idx),selected.to_dense())\n",
    "\n",
    "print(sparse_matrix)\n",
    "# SparseTensor(indices=tensor([[0, 0], [1, 1], [0, 2], [2, 0]]),  \n",
    "#              values=tensor([2, 5, 8, 10]),\n",
    "#              size=(3, 3), nnz=4, layout=torch.sparse_coo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs_gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
